## Questions

## Neural Networks and Deep Learning**

1. What input/hidden/output layers of NN are ?
2. Describe the math of single Neuron.
3. Activation function. Types of activation functions.
4. Why do you need non-linear activation function?
5.  Pros and Cons of sigmoid, tanh, ReLU, Leaky ReLU
6. Backpropogation simple explanation
[image:A34136FE-9031-4CA6-A21E-F9FCD0E5D492-509-0000B85B6DFC30FB/AC78FD38-C896-4654-B6FA-B4520BE8797C.png]
7. Why do you need random initialization in NN?


## Questions from quiz

8. The tanh activation usually works better than sigmoid activation function for hidden units because the mean of its output is closer to zero, and so it centers the data better for the next layer. True/False?
9. Suppose you have built a neural network. What will happen if you decide to initialize the weights and biases to be zero?
10. You have built a network using the tanh activation for all the hidden units. You initialize the weights to relative large values, using np.random.randn(..,..)*1000. What will happen?
