## Questions

## **Hyperparameter tuning, Batch Normalization and Programming Frameworks**

1. Batch normalization. The formula for batch norm. Why does it work (intuition)?
[image]
2. What is the purpose of gamma and beta in Batch Norm? How they can be learned?
3. The process of fitting batch norm to NN network.
[image]
4. How you can use Batch Norm as regularization.
5. Softmax regression. Softmax layer. The math behind it.
[image]


## Questions from quiz

6. **If searching among a large number of hyperparameters, you should try values in a grid rather than random values, so that you can carry out the search more systematically and not rely on chance. True or False?**
7. **Every hyperparameter, if set poorly, can have a huge negative impact on training, and so all hyperparameters are about equally important to tune well. True or False?**
8. **In the normalization formula z(i)norm=z(i)−μσ2+ε√, why do we use epsilon?**

