## Questions

## **Hyperparameter tuning, Batch Normalization and Programming Frameworks**

1. Batch normalization. The formula for batch norm. Why does it work (intuition)?
![BN](https://github.com/OzmundSedler/Deep-Learning-Coursera/blob/master/2%20Improving%20Deep%20Neural%20Networks%20Hyperparameter%20tuning%2C%20Regularization%20and%20Optimization/week%203/images/Screenshot%202020-05-12%20at%2008.31.13.png)
2. What is the purpose of gamma and beta in Batch Norm? How they can be learned?
3. The process of fitting batch norm to NN network.
![BN3](https://github.com/OzmundSedler/Deep-Learning-Coursera/blob/master/2%20Improving%20Deep%20Neural%20Networks%20Hyperparameter%20tuning%2C%20Regularization%20and%20Optimization/week%203/images/Screenshot%202020-05-12%20at%2008.32.56.png)
4. How you can use Batch Norm as regularization.
5. Softmax regression. Softmax layer. The math behind it.
![Softmax](https://github.com/OzmundSedler/Deep-Learning-Coursera/blob/master/2%20Improving%20Deep%20Neural%20Networks%20Hyperparameter%20tuning%2C%20Regularization%20and%20Optimization/week%203/images/Screenshot%202020-05-12%20at%2008.33.47.png)

## Questions from the quiz

6. If searching among a large number of hyperparameters, you should try values in a grid rather than random values, so that you can carry out the search more systematically and not rely on chance. True or False?
7. Every hyperparameter, if set poorly, can have a huge negative impact on training, and so all hyperparameters are about equally important to tune well. True or False?
8. In the normalization formula z(i)norm=z(i)−μσ2+ε√, why do we use epsilon?

