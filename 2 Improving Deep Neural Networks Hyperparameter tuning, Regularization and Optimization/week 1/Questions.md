## Questions

## **Practical aspects of Deep Learning**

1. Train/test/dev sets. How will you split 10k dataset. 1m dataset?
2. Explain bias/Variance tradeoff.
3. Types of regularization. NN example. Why regularization reduces overfitting?
[image]
4. Explain dropout regularization. What is the principle behind it. Why does it works?
5. What is data augmentation and how it is used?
6. What is early stopping and how it is used?
7. Why do you need input normalization in NN?
8. Explain vanishing/exploding gradients problem.
9. How you need to initialize weights in Deep NN? What types of initialization do you know?
10. Gradient checking
[image]
[image]


## Questions from quiz
11. If your Neural Network model seems to have high bias, what will you try?
12. You are working on an automated check-out kiosk for a supermarket, and are building a classifier for apples, bananas and oranges. Suppose your classifier obtains a training set error of 0.5%, and a dev set error of 7%. What will you try to improve your classifier?
13. What is weight decay?
14. What happens when you continue to increase the regularization hyperparameter lambda?
15. With the inverted dropout technique, what will you do at test with your NN?
16. Increasing the parameter keep_prob from (say) 0.5 to 0.6 in dropout will likely cause the following:
- ...... the regularization effect
- Causing the neural network to end up with a ... training set error
17. Give examples of techniques useful for reducing variance (reducing overfitting)?
18. Why do we normalize the inputs x?