## Questions

**Natural Language Processing & Word Embeddings**

1. What is word embeddings ?
2. How transfer learning is connected to the word embeddings?
3. What is finetuning?
4. Cosine similarity. Formula. Where it can be applied?
5. Bag of words algorithm
6. A neural probabilistic language model 
![image]() 
![image2]()
7. What is skip-gram model?
8. Word2Vec algorithm. 
9. Intuition behind negative sampling
![image3]()
10. GloVe word vectors 
![GloVe]() 
![GloVe2]()
11. Give an example of sentiment classification model and how it can be applied.

**More from the quiz**

12. Suppose you learn a word embedding for a vocabulary of 10000 words. Then the embedding vectors should be 10000 dimensional, so as to capture the full range of variation and meaning in those words. True/False?
13. Suppose you download a pre-trained word embedding which has been trained on a huge corpus of text. You then use this word embedding to train an RNN for a language task of recognizing if someone is happy from a short snippet of text, using a small training set.
    Then if the word “ecstatic” does not appear in your small training set, how your RNN will recognize “I’m ecstatic” and what label it will probably assign to it? Why?
14. Give an example of equations you think should hold for a good word embedding. (e_{boy} - e_{girl} ≈ e_{brother} - e_{sister})
15. When learning word embeddings, we create an artificial task of estimating P(target \mid context)P(target∣context). It is okay if we do poorly on this artificial prediction task; the more important by-product of this task is that we learn a useful set of word embeddings. True/False?
16. In the word2vec algorithm, you estimate P(t∣c), where tt is the target word and cc is a context word. How are tt and cc chosen from the training set? 

